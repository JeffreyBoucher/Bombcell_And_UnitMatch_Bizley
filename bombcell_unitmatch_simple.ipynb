{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete BombCell + UnitMatch Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for neural spike analysis using BombCell and UnitMatch:\n",
    "\n",
    "## ğŸ¯ **What This Demo Does**\n",
    "\n",
    "### Part 1: BombCell Quality Control\n",
    "- Analyzes spike sorting data from Kilosort\n",
    "- Extracts comprehensive quality metrics for each unit\n",
    "- Classifies units as \"GOOD\", \"MUA\", \"NOISE\", or \"NON-SOMA\"  \n",
    "- **Creates waveform data specifically formatted for UnitMatch**\n",
    "\n",
    "### Part 2: UnitMatch Cross-Session Tracking\n",
    "- Uses BombCell outputs to track neurons across recording sessions\n",
    "- Applies machine learning to match units based on waveform features\n",
    "- Provides interactive tools for manual curation\n",
    "- Generates probability matrices showing match confidence\n",
    "\n",
    "## ğŸ”— **Key Integration Points**\n",
    "- BombCell creates `RawWaveforms/` folders that UnitMatch requires\n",
    "- Special parameters optimize BombCell for UnitMatch compatibility\n",
    "- Quality classifications help filter units for reliable matching\n",
    "- Both tools work seamlessly with standard Kilosort outputs\n",
    "\n",
    "## ğŸ“‹ **What You Need**\n",
    "- Kilosort output directories from 2+ recording sessions if you want to track across days or for 1 recording session if you want to merge units intra-session\n",
    "- Access to raw `.bin` files (for waveform extraction) - can be compressed `.cbin` files\n",
    "- Meta files (`.meta` for SpikeGLX or `.oebin` for Open Ephys) ### the fact I need meta files is kinda unfortunate for concatenated stuff. Specifically because I need to either create one anew or I need to fake one that has the required info but no more.\n",
    "\n",
    "## ğŸ’¡ **Important for Compressed Data**\n",
    "If your data is in compressed format (e.g., `.cbin` files from mtscomp), the pipeline will automatically handle decompression:\n",
    "- BombCell's UnitMatch functions use `decompress_data=True` by default\n",
    "- Install the mtscomp package: `pip install mtscomp`\n",
    "- This enables automatic decompression during waveform extraction\n",
    "\n",
    "## ğŸš€ **Getting Started**\n",
    "Simply run the cells below to analyze your own data - just update the file paths in Part 1!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload \n",
    "\n",
    "# ğŸ“¦ Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os_true\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import BombCell for quality control\n",
    "import bombcell as bc\n",
    "\n",
    "# Import UnitMatch for cross-session tracking (optional - only if running Part 2)\n",
    "try:\n",
    "    import UnitMatchPy.bayes_functions as bf\n",
    "    import UnitMatchPy.utils as util\n",
    "    import UnitMatchPy.overlord as ov\n",
    "    import UnitMatchPy.save_utils as su\n",
    "    import UnitMatchPy.GUI as um_gui\n",
    "    import UnitMatchPy.assign_unique_id as aid\n",
    "    import UnitMatchPy.default_params as default_params\n",
    "    UNITMATCH_AVAILABLE = True\n",
    "    print(\"âœ… BombCell and UnitMatch imported successfully\")\n",
    "except ImportError as e:\n",
    "    UNITMATCH_AVAILABLE = False\n",
    "    print(\"âœ… BombCell imported successfully\")\n",
    "    print(\"âš ï¸  UnitMatch not available - please install: pip install UnitMatchPy\")\n",
    "    print(f\"    Error: {e}\")\n",
    "\n",
    "print(\"ğŸš€ Ready to analyze neural data!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Part 1: Configure Data Paths for BombCell\n",
    "\n",
    "**Edit these paths to point to your actual data:**\n",
    "\n",
    "For this demo, we'll analyze data from multiple sessions to demonstrate the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ğŸ“ Configure Data Paths - Using Processing Playground Dataset\n",
    "\n",
    "# These are the exact paths from the processing_playground for testing\n",
    "# calca_302 sessions from 2023-04-19 and 2023-04-20\n",
    "isJeff = True\n",
    "\n",
    "if isJeff:\n",
    "    ### will need to plug in an external harddrive and point to it, I think...\n",
    "    session_configs = [\n",
    "        {\n",
    "            'name': 'weekOf20240513_PFC_F2302_Challah', ### just made this up\n",
    "            'ks_dir': r\"F:\\Jeff\\Ferrets_Spikesorted\\F2302_Challah\\spikesorted\\tempDir\\PFC_shank0_Challah\\PFC_shank0\\weekOf20240513\\tempDir\\kilosort4_output\\sorter_output\", ### copy location\n",
    "            'raw_file': r\"D:\\F2302_Challah_temp\\13052024_AM_Challah_g0\\13052024_AM_Challah_g0_imec1\\13052024_AM_Challah_g0_t0.imec1.ap.bin\", ### don't have yet, will be on an external drive\n",
    "            'meta_file': r\"None\", ### might never exist. Might have to make a fake one.\n",
    "            'save_path': r\"F:\\Jeffrey\\BombcellTestOutput\" ### Louise set this as the same as ks_dir... The way I am doing it I might make a clone.\n",
    "        },\n",
    "        # {\n",
    "        #     'name': '13052024_PM_Challah_g0',\n",
    "        #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "        #     'raw_file': r\"D:\\F2302_Challah_temp\\13052024_PM_Challah_g0\\13052024_PM_Challah_g0_imec1\\13052024_PM_Challah_g0_t0.imec1.ap.bin\",\n",
    "        #     'meta_file': r\"D:\\F2302_Challah_temp\\13052024_PM_Challah_g0\\13052024_PM_Challah_g0_imec1\\13052024_PM_Challah_g0_t0.imec1.ap.meta\",\n",
    "        #     'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "        # },\n",
    "        # {\n",
    "        #     'name': '14052024_AM_Challah_g0',\n",
    "        #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "        #     'raw_file': r\"D:\\F2302_Challah_temp\\14052024_AM_Challah_g0\\14052024_AM_Challah_g0_imec1\\14052024_AM_Challah_g0_t0.imec1.ap.bin\",\n",
    "        #     'meta_file': r\"D:\\F2302_Challah_temp\\14052024_AM_Challah_g0\\14052024_AM_Challah_g0_imec1\\14052024_AM_Challah_g0_t0.imec1.ap.meta\",\n",
    "        #     'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "        # }#,\n",
    "    ]\n",
    "else: ### here are the configs which\n",
    "    session_configs = [\n",
    "        {\n",
    "            'name': '13052024_AM_Challah_g0',\n",
    "            'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "            'raw_file': r\"D:\\F2302_Challah_temp\\13052024_AM_Challah_g0\\13052024_AM_Challah_g0_imec1\\13052024_AM_Challah_g0_t0.imec1.ap.bin\",\n",
    "            'meta_file': r\"D:\\F2302_Challah_temp\\13052024_AM_Challah_g0\\13052024_AM_Challah_g0_imec1\\13052024_AM_Challah_g0_t0.imec1.ap.meta\",\n",
    "            'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "        },\n",
    "        {\n",
    "            'name': '13052024_PM_Challah_g0',\n",
    "            'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "            'raw_file': r\"D:\\F2302_Challah_temp\\13052024_PM_Challah_g0\\13052024_PM_Challah_g0_imec1\\13052024_PM_Challah_g0_t0.imec1.ap.bin\",\n",
    "            'meta_file': r\"D:\\F2302_Challah_temp\\13052024_PM_Challah_g0\\13052024_PM_Challah_g0_imec1\\13052024_PM_Challah_g0_t0.imec1.ap.meta\",\n",
    "            'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "        },\n",
    "        {\n",
    "            'name': '14052024_AM_Challah_g0',\n",
    "            'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "            'raw_file': r\"D:\\F2302_Challah_temp\\14052024_AM_Challah_g0\\14052024_AM_Challah_g0_imec1\\14052024_AM_Challah_g0_t0.imec1.ap.bin\",\n",
    "            'meta_file': r\"D:\\F2302_Challah_temp\\14052024_AM_Challah_g0\\14052024_AM_Challah_g0_imec1\\14052024_AM_Challah_g0_t0.imec1.ap.meta\",\n",
    "            'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "        }#,\n",
    "    # {\n",
    "    #     'name': '14052024_PM_Challah_g0',\n",
    "    #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "    #     'raw_file': r\"D:\\F2302_Challah_temp\\14052024_PM_Challah_g0\\14052024_PM_Challah_g0_imec1\\14052024_PM_Challah_g0_t0.imec1.ap.bin\",\n",
    "    #     'meta_file': r\"D:\\F2302_Challah_temp\\14052024_PM_Challah_g0\\14052024_PM_Challah_g0_imec1\\14052024_PM_Challah_g0_t0.imec1.ap.meta\",\n",
    "    #     'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '16052024_AM_Challah_g0',\n",
    "    #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\16052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "    #     'raw_file': r\"D:\\F2302_Challah_temp\\16052024_AM_Challah_g0\\16052024_AM_Challah_g0_imec1\\16052024_AM_Challah_g0_t0.imec1.ap.bin\",\n",
    "    #     'meta_file': r\"D:\\F2302_Challah_temp\\16052024_AM_Challah_g0\\16052024_AM_Challah_g0_imec1\\16052024_AM_Challah_g0_t0.imec1.ap.meta\",\n",
    "    #     'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\16052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '16052024_PM_Challah_g0',\n",
    "    #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\16052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "    #     'raw_file': r\"D:\\F2302_Challah_temp\\16052024_PM_Challah_g0\\16052024_PM_Challah_g0_imec1\\16052024_PM_Challah_g0_t0.imec1.ap.bin\",\n",
    "    #     'meta_file': r\"D:\\F2302_Challah_temp\\16052024_PM_Challah_g0\\16052024_PM_Challah_g0_imec1\\16052024_PM_Challah_g0_t0.imec1.ap.meta\",\n",
    "    #     'save_path': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\16052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '17052024_AM_Challah_g0',\n",
    "    #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\17052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "    #     'raw_file': r\"D:\\F2302_Challah_temp\\17052024_AM_Challah_g0\\17052024_AM_Challah_g0_imec1\\17052024_AM_Challah_g0_t0.imec1.ap.bin\",\n",
    "    #     'meta_file': r\"D:\\F2302_Challah_temp\\17052024_AM_Challah_g0\\17052024_AM_Challah_g0_imec1\\17052024_AM_Challah_g0_t0.imec1.ap.meta\",\n",
    "    #     'save_path' : r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\17052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': '17052024_AM_Challah_2_g0',\n",
    "    #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\17052024_AM_Challah_2_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "    #     'raw_file': r\"D:\\F2302_Challah_temp\\17052024_AM_Challah_2_g0\\17052024_AM_Challah_2_g0_imec1\\17052024_AM_Challah_2_g0_t0.imec1.ap.bin\",\n",
    "    #     'meta_file': r\"D:\\F2302_Challah_temp\\17052024_AM_Challah_2_g0\\17052024_AM_Challah_2_g0_imec1\\17052024_AM_Challah_2_g0_t0.imec1.ap.meta\",\n",
    "    #     'save_path' : r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\17052024_AM_Challah_2_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "    # },\n",
    "    #\n",
    "    # {\n",
    "    #     'name': '17052024_PM_Challah_g0',\n",
    "    #     'ks_dir': r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\17052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\",\n",
    "    #     'raw_file': r\"D:\\F2302_Challah_temp\\17052024_PM_Challah_g0\\17052024_PM_Challah_g0_imec1\\17052024_PM_Challah_g0_t0.imec1.ap.bin\",\n",
    "    #     'meta_file': r\"D:\\F2302_Challah_temp\\17052024_PM_Challah_g0\\17052024_PM_Challah_g0_imec1\\17052024_PM_Challah_g0_t0.imec1.ap.meta\",\n",
    "    #     'save_path' : r\"F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\17052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\"\n",
    "    # }\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kilosort_version = 4\n",
    "\n",
    "print(\"ğŸ¯ BombCell + UnitMatch Pipeline Demo\")\n",
    "print(\"ğŸ“Š Dataset: calca_302 cross-session tracking\")\n",
    "print(f\"ğŸ”¬ Sessions to analyze: {len(session_configs)}\")\n",
    "for i, config in enumerate(session_configs):\n",
    "    print(f\"   Session {i+1}: {config['name']}\")\n",
    "    print(f\"      ğŸ“ KS: {Path(config['ks_dir']).name}\")\n",
    "    \n",
    "print(f\"\\nğŸ”§ Kilosort version: {kilosort_version}\")\n",
    "print(\"ğŸ¯ This demo will run BombCell first, then use outputs for UnitMatch tracking\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Part 1: Run BombCell Quality Control\n",
    "\n",
    "**What BombCell does:**\n",
    "- Analyzes each unit's waveform properties, firing patterns, and spatial characteristics\n",
    "- Calculates 15+ quality metrics (amplitude, drift, contamination, etc.)\n",
    "- Classifies units into categories based on quality thresholds\n",
    "- **Special for UnitMatch**: Extracts 1000 raw spikes per unit (vs. standard 100)\n",
    "\n",
    "**Key UnitMatch optimizations:**\n",
    "- `detrendWaveform=False`: Preserves raw waveform shape for matching\n",
    "- `nRawSpikesToExtract=1000`: More spikes = better cross-validation\n",
    "- `saveMultipleRaw=True`: Saves separate waveform sets for validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_bombcell_session(session_config):\n",
    "    \"\"\"Run BombCell on a single session with UnitMatch parameters\"\"\"\n",
    "    \n",
    "    name = session_config['name']\n",
    "    ks_dir = session_config['ks_dir']\n",
    "    raw_file = session_config.get('raw_file')\n",
    "    meta_file = session_config.get('meta_file')\n",
    "    \n",
    "    # Create save path in the kilosort directory\n",
    "    save_path = Path(session_config['save_path'])\n",
    "    \n",
    "    print(f\"ğŸ”¬ Analyzing session: {name}\")\n",
    "    print(f\"   ğŸ“ Kilosort directory: {ks_dir}\")\n",
    "    print(f\"   ğŸ—‚ï¸  Raw file: {Path(raw_file).name if raw_file else 'Not specified'}\")\n",
    "    print(f\"   ğŸ“„ Meta file: {Path(meta_file).name if meta_file else 'Not specified'}\")\n",
    "    print(f\"   ğŸ’¾ Results will be saved to: {save_path}\")\n",
    "    \n",
    "    # Check if BombCell has already been run\n",
    "    existing_results = save_path / \"cluster_bc_unitType.tsv\"\n",
    "    if existing_results.exists():\n",
    "        print(f\"   âœ… Found existing BombCell results - loading from disk\")\n",
    "        \n",
    "        # Load existing results\n",
    "        try:\n",
    "            param, quality_metrics, unit_type_string = bc.load_bc_results(str(save_path))\n",
    "            unit_type = np.array([1 if ut == 'GOOD' else 0 for ut in unit_type_string])\n",
    "            \n",
    "            print(f\"   ğŸ“Š Loaded results - Total units: {len(unit_type)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸  Error loading existing results: {e}\")\n",
    "            print(f\"   ğŸ”„ Will re-run BombCell analysis...\")\n",
    "            existing_results = None\n",
    "    else:\n",
    "        existing_results = None\n",
    "    \n",
    "    # Run BombCell if no existing results or loading failed\n",
    "    if not existing_results or not existing_results.exists():\n",
    "        print(\"   ğŸš€ Running BombCell analysis...\")\n",
    "        quality_metrics, param, unit_type, unit_type_string = bc.run_bombcell_unit_match(\n",
    "            ks_dir=ks_dir,\n",
    "            save_path=str(save_path),\n",
    "            raw_file=raw_file,\n",
    "            meta_file=meta_file,\n",
    "            kilosort_version=kilosort_version\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Analysis complete!\")\n",
    "        print(f\"   ğŸ“Š Total units: {len(unit_type)}\")\n",
    "\n",
    "    \n",
    "    # Check UnitMatch waveforms\n",
    "    raw_waveforms_dir = save_path / \"RawWaveforms\"\n",
    "    if raw_waveforms_dir.exists():\n",
    "        waveform_files = list(raw_waveforms_dir.glob(\"Unit*_RawSpikes.npy\"))\n",
    "        print(f\"   ğŸ¯ UnitMatch waveforms: {len(waveform_files)} files saved\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  No UnitMatch waveforms - check raw file access\")\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'ks_dir': ks_dir,\n",
    "        'save_path': str(save_path),\n",
    "        'quality_metrics': quality_metrics,\n",
    "        'param': param,\n",
    "        'unit_type': unit_type,\n",
    "        'unit_type_string': unit_type_string\n",
    "    }\n",
    "\n",
    "# Run BombCell on all sessions\n",
    "print(\"ğŸš€ Running BombCell analysis on all sessions...\")\n",
    "session_results = []\n",
    "\n",
    "for i, session_config in enumerate(session_configs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SESSION {i+1}/{len(session_configs)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    result = run_bombcell_session(session_config)\n",
    "    session_results.append(result)\n",
    "\n",
    "print(f\"\\nğŸ‰ BombCell analysis complete for {len(session_results)} sessions!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Part 2: UnitMatch Cross-Session Tracking\n",
    "\n",
    "**What UnitMatch does:**\n",
    "- Compares units across different recording sessions using multiple features\n",
    "- Uses machine learning (Naive Bayes) to calculate match probabilities  \n",
    "- Identifies the same neurons recorded on different days\n",
    "- Provides tools for manual curation and validation\n",
    "\n",
    "**This section requires multiple sessions with BombCell outputs!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ğŸ”— Setup UnitMatch Parameters and Paths\n",
    "\n",
    "print(\"ğŸ¯ Setting up UnitMatch for cross-session tracking...\")\n",
    "\n",
    "# Get default UnitMatch parameters\n",
    "um_param = default_params.get_default_param()\n",
    "\n",
    "# Set up paths from our BombCell results\n",
    "KS_dirs = [result['ks_dir'] for result in session_results]\n",
    "um_param['KS_dirs'] = KS_dirs\n",
    "\n",
    "# Set up BombCell paths - using exact paths from processing_playground\n",
    "custom_bombcell_paths = [\n",
    "    r'F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\\cluster_bc_unitType.tsv',\n",
    "    r'F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\\cluster_bc_unitType.tsv',\n",
    "    r'F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\\cluster_bc_unitType.tsv'\n",
    "]\n",
    "\n",
    "custom_raw_waveform_paths = [\n",
    "    r'F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\\RawWaveforms',\n",
    "    r'F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\13052024_PM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\\RawWaveforms',\n",
    "    r'F:\\Louise\\Output\\tempDir\\F2302_Challah\\PFC_shank0_Challah\\PFC_shank0\\everythingAllAtOnce\\14052024_AM_Challah_g0\\tempDir\\kilosort4_output\\sorter_output\\RawWaveforms'\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“ Kilosort directories: {len(KS_dirs)}\")\n",
    "for i, ks_dir in enumerate(KS_dirs):\n",
    "    print(f\"   Session {i+1}: {Path(ks_dir).name}\")\n",
    "\n",
    "print(f\"ğŸ“Š BombCell unit classifications:\")\n",
    "for i, bc_path in enumerate(custom_bombcell_paths):\n",
    "    exists = \"âœ…\" if Path(bc_path).exists() else \"âŒ\"\n",
    "    print(f\"   Session {i+1}: {exists} {Path(bc_path).name}\")\n",
    "    \n",
    "print(f\"ğŸ¯ Raw waveforms for UnitMatch:\")\n",
    "for i, wv_path in enumerate(custom_raw_waveform_paths):\n",
    "    exists = \"âœ…\" if Path(wv_path).exists() else \"âŒ\"\n",
    "    n_files = len(list(Path(wv_path).glob(\"Unit*_RawSpikes.npy\"))) if Path(wv_path).exists() else 0\n",
    "    print(f\"   Session {i+1}: {exists} {n_files} waveform files\")\n",
    "\n",
    "# Setup UnitMatch paths - this matches exactly what processing_playground does\n",
    "try:\n",
    "    wave_paths, unit_label_paths, channel_pos = util.paths_from_KS(                #\n",
    "        KS_dirs,\n",
    "        custom_raw_waveform_paths=custom_raw_waveform_paths,\n",
    "        custom_bombcell_paths=custom_bombcell_paths\n",
    "    )\n",
    "    \n",
    "    um_param = util.get_probe_geometry(channel_pos[0], um_param)\n",
    "    print(\"âœ… UnitMatch paths configured successfully\")\n",
    "    UNITMATCH_READY = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error setting up UnitMatch paths: {e}\")\n",
    "    print(\"   Make sure BombCell has been run and waveforms extracted\")\n",
    "    UNITMATCH_READY = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ğŸ§  Run UnitMatch Analysis Pipeline\n",
    "\n",
    "\n",
    "print(\"ğŸš€ Running UnitMatch cross-session analysis...\")\n",
    "\n",
    "# Step 0: Load good units and waveform data\n",
    "print(\"ğŸ“Š Step 0: Loading unit waveforms...\")\n",
    "waveform, session_id, session_switch, within_session, good_units, um_param = util.load_good_waveforms(\n",
    "    wave_paths, unit_label_paths, um_param, good_units_only=True\n",
    ") \n",
    "\n",
    "print(f\"   âœ… Loaded {len(np.concatenate(good_units))} good units across {len(session_results)} sessions\")\n",
    "print(f\"   ğŸ“ Waveform shape: {waveform.shape} (units Ã— time Ã— channels)\")\n",
    "\n",
    "# Create cluster info\n",
    "clus_info = {\n",
    "    'good_units': good_units, #modified to Good and MUA\n",
    "    'session_switch': session_switch, \n",
    "    'session_id': session_id,\n",
    "    'original_ids': np.concatenate(good_units)\n",
    "}\n",
    "\n",
    "# Step 1: Extract waveform parameters\n",
    "print(\"ğŸ” Step 1: Extracting waveform features...\")\n",
    "extracted_wave_properties = ov.extract_parameters(waveform, channel_pos, clus_info, um_param)\n",
    "print(\"   âœ… Extracted amplitude, spatial decay, and waveform features\")\n",
    "\n",
    "# Steps 2-4: Calculate similarity metrics with drift correction\n",
    "print(\"ğŸ“ Steps 2-4: Computing similarity metrics and drift correction...\")\n",
    "total_score, candidate_pairs, scores_to_include, predictors = ov.extract_metric_scores(\n",
    "    extracted_wave_properties, session_switch, within_session, um_param, niter=2\n",
    ")\n",
    "print(f\"   âœ… Found {np.sum(candidate_pairs)} candidate unit pairs\")\n",
    "print(f\"   ğŸ“Š Metrics included: {list(scores_to_include.keys())}\")\n",
    "\n",
    "# Step 5: Probability analysis using Naive Bayes\n",
    "print(\"ğŸ§® Step 5: Calculating match probabilities...\")\n",
    "\n",
    "# Set up priors\n",
    "prior_match = 1 - (um_param['n_expected_matches'] / um_param['n_units']**2)\n",
    "priors = np.array([prior_match, 1 - prior_match])\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "labels = candidate_pairs.astype(int)\n",
    "cond = np.unique(labels)\n",
    "parameter_kernels = bf.get_parameter_kernels(scores_to_include, labels, cond, um_param, add_one=1)\n",
    "\n",
    "# Calculate match probabilities\n",
    "probability = bf.apply_naive_bayes(parameter_kernels, priors, predictors, um_param, cond)\n",
    "output_prob_matrix = probability[:, 1].reshape(um_param['n_units'], um_param['n_units'])\n",
    "\n",
    "print(f\"   âœ… Calculated probability matrix: {output_prob_matrix.shape}\")\n",
    "print(f\"   ğŸ“ˆ Max probability: {np.max(output_prob_matrix):.3f}\")\n",
    "print(f\"   ğŸ“Š Mean probability: {np.mean(output_prob_matrix):.3f}\")\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ğŸ“Š Analyze UnitMatch Results\n",
    "\n",
    "\n",
    "print(\"ğŸ“ˆ Analyzing UnitMatch results...\")\n",
    "\n",
    "# Evaluate matching performance\n",
    "match_threshold = um_param.get('match_threshold', 0.75)\n",
    "util.evaluate_output(output_prob_matrix, um_param, within_session, session_switch, \n",
    "                    match_threshold=match_threshold)\n",
    "\n",
    "# Create binary match matrix\n",
    "output_threshold = np.zeros_like(output_prob_matrix)\n",
    "output_threshold[output_prob_matrix > match_threshold] = 1\n",
    "\n",
    "# Count matches\n",
    "total_matches = np.sum(output_threshold)\n",
    "within_session_matches = np.sum(output_threshold * within_session)\n",
    "cross_session_matches = total_matches - within_session_matches\n",
    "\n",
    "print(f\"\\nğŸ¯ Match Summary (threshold = {match_threshold}):\")\n",
    "print(f\"   ğŸ”— Total matches found: {total_matches}\")\n",
    "print(f\"   ğŸ“ Within-session matches: {within_session_matches}\")\n",
    "print(f\"   ğŸŒ‰ Cross-session matches: {cross_session_matches}\")\n",
    "\n",
    "# Visualize probability matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Probability matrix\n",
    "im1 = axes[0].imshow(output_prob_matrix, cmap='viridis', aspect='auto')\n",
    "axes[0].set_title('Unit Match Probability Matrix')\n",
    "axes[0].set_xlabel('Unit Index')\n",
    "axes[0].set_ylabel('Unit Index')\n",
    "plt.colorbar(im1, ax=axes[0], label='Match Probability')\n",
    "\n",
    "# Session boundaries\n",
    "n_units_cumsum = np.cumsum([0] + [len(units) for units in good_units])\n",
    "for boundary in n_units_cumsum[1:-1]:\n",
    "    axes[0].axhline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0].axvline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Binary matches\n",
    "im2 = axes[1].imshow(output_threshold, cmap='Greys', aspect='auto')\n",
    "axes[1].set_title(f'Matches Above Threshold ({match_threshold})')\n",
    "axes[1].set_xlabel('Unit Index')\n",
    "axes[1].set_ylabel('Unit Index')\n",
    "\n",
    "# Session boundaries\n",
    "for boundary in n_units_cumsum[1:-1]:\n",
    "    axes[1].axhline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1].axvline(boundary, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Red dashed lines show session boundaries\")\n",
    "print(\"ğŸ¯ Diagonal represents within-session matches\")\n",
    "print(\"ğŸŒ‰ Off-diagonal represents cross-session matches\")\n",
    "\n",
    "print(um_param)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ğŸ® Interactive Manual Curation with UnitMatch GUI\n",
    "\n",
    "\n",
    "print(\"ğŸ® Setting up interactive curation tools...\")\n",
    "\n",
    "# Prepare data for GUI - extract all the required variables\n",
    "amplitude = extracted_wave_properties['amplitude']\n",
    "spatial_decay = extracted_wave_properties['spatial_decay']\n",
    "avg_centroid = extracted_wave_properties['avg_centroid']\n",
    "avg_waveform = extracted_wave_properties['avg_waveform']\n",
    "avg_waveform_per_tp = extracted_wave_properties['avg_waveform_per_tp']\n",
    "wave_idx = extracted_wave_properties['good_wave_idxs']\n",
    "max_site = extracted_wave_properties['max_site']\n",
    "max_site_mean = extracted_wave_properties['max_site_mean']\n",
    "\n",
    "# Process info for GUI\n",
    "um_gui.process_info_for_GUI(\n",
    "    output_prob_matrix, match_threshold, scores_to_include, total_score, \n",
    "    amplitude, spatial_decay, avg_centroid, avg_waveform, avg_waveform_per_tp, \n",
    "    wave_idx, max_site, max_site_mean, waveform, within_session, \n",
    "    channel_pos, clus_info, um_param\n",
    ")\n",
    "\n",
    "print(\"âœ… GUI data prepared successfully\")\n",
    "print(\"\\nğŸ¯ To launch interactive curation:\")\n",
    "print(\"   Run: is_match, not_match, matches_GUI = um_gui.run_GUI()\")\n",
    "print(\"\\nğŸ” The GUI provides:\")\n",
    "print(\"   - Interactive visualization of unit pairs\")\n",
    "print(\"   - Side-by-side waveform comparisons\")\n",
    "print(\"   - Quality metric displays\")\n",
    "print(\"   - Manual accept/reject controls\")\n",
    "print(\"   - Real-time match probability updates\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Curation workflow:\")\n",
    "print(\"   1. GUI shows potential matches above threshold\")\n",
    "print(\"   2. Review waveform similarity and spatial locations\")\n",
    "print(\"   3. Accept good matches, reject false positives\")\n",
    "print(\"   4. Use curated results for final unit tracking\")\n",
    "\n",
    "# Before running the GUI\n",
    "from UnitMatchPy.GUI import precalculate_all_acgs ### does not seem to actually exist?\n",
    "\n",
    "# Pre-calculate and save ACGs\n",
    "\n",
    "acg_cache = precalculate_all_acgs(clus_info, um_param,\n",
    "save_path='my_acgs.pkl')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "is_match, not_match, matches_GUI = um_gui.run_GUI()\n",
    "# GUI guide: https://github.com/EnnyvanBeest/UnitMatch/blob/main/UnitMatchPy/Demo%20Notebooks/GUI_Reference_Guide.md"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ğŸ’¾ Save Results and Generate Final Outputs\n",
    "\n",
    "print(\"ğŸ’¾ Saving UnitMatch results and generating final outputs...\")\n",
    "\n",
    "# Assign unique IDs to matched units across sessions\n",
    "UIDs = aid.assign_unique_id(output_prob_matrix, um_param, clus_info)\n",
    "\n",
    "# Get final matches above threshold\n",
    "matches = np.argwhere(output_threshold == 1)\n",
    "\n",
    "# Create save directory next to the first session's BombCell results\n",
    "save_dir = Path(session_results[0]['save_path']).parent / \"UnitMatch_Results\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ Saving results to: {save_dir}\")\n",
    "\n",
    "# Save comprehensive UnitMatch results\n",
    "su.save_to_output(\n",
    "    str(save_dir), \n",
    "    scores_to_include, \n",
    "    matches,\n",
    "    output_prob_matrix, \n",
    "    avg_centroid, \n",
    "    avg_waveform, \n",
    "    avg_waveform_per_tp, \n",
    "    max_site,\n",
    "    total_score, \n",
    "    output_threshold, \n",
    "    clus_info, \n",
    "    um_param, \n",
    "    UIDs=UIDs, \n",
    "    matches_curated=None,  # Set to matches_curated if manual curation was performed\n",
    "    save_match_table=True\n",
    ")\n",
    "\n",
    "print(\"âœ… Results saved successfully!\")\n",
    "\n",
    "# Generate summary statistics\n",
    "n_unique_neurons = len(np.unique(UIDs))\n",
    "n_total_units = len(np.concatenate(good_units))\n",
    "n_cross_session_matches = cross_session_matches\n",
    "\n",
    "print(f\"\\nğŸ“Š Final Results Summary:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ğŸ§  Dataset: calca_302 (2 sessions)\")\n",
    "print(f\"ğŸ“Š Total units analyzed: {sum(len(result['unit_type']) for result in session_results)}\")\n",
    "print(f\"âœ… Good units tracked: {n_total_units}\")\n",
    "print(f\"ğŸ”— Cross-session matches found: {n_cross_session_matches}\")\n",
    "print(f\"ğŸ·ï¸  Unique neurons identified: {n_unique_neurons}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Output Files Generated:\")\n",
    "print(f\"\\nğŸ“ Output Files Generated:\")\n",
    "print(f\"   ğŸ“Š MatchProb.npy - Match probability matrix\")\n",
    "print(f\"   ğŸ“‹ match_table.csv - Detailed match information\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ Pipeline Complete!\n",
    "\n",
    "\n",
    "### Key Files Generated\n",
    "\n",
    "**BombCell Outputs (per session):**\n",
    "- `qMetrics/cluster_bc_unitType.tsv` - Unit classifications\n",
    "- `qMetrics/templates._bc_qMetrics.parquet` - Quality metrics\n",
    "- `qMetrics/RawWaveforms/` - Raw spike waveforms for UnitMatch\n",
    "- `qMetrics/summary_plots.png` - Quality control visualizations\n",
    "\n",
    "**UnitMatch Outputs:**\n",
    "- `UnitMatch_Results/output_prob_matrix.npy` - Match probabilities\n",
    "- `UnitMatch_Results/match_table.csv` - Unit matches and scores\n",
    "- `UnitMatch_Results/unique_IDs.npy` - Cross-session unit identifiers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
